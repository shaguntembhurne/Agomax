{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6efab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3000\n",
      "Test samples: 2000\n",
      "Injected anomalies: 400\n",
      "Anomaly start index: 1600\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Synthetic Drone Telemetry Generator\n",
    "# Creates:\n",
    "#   - train_normal.csv (NORMAL ONLY)\n",
    "#   - test_with_anomalies.csv (NORMAL + ANOMALIES)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "TRAIN_SAMPLES = 3000\n",
    "TEST_SAMPLES = 2000\n",
    "ANOMALY_START = 1600   # anomalies begin here (end region)\n",
    "\n",
    "# ============================================================\n",
    "# NORMAL FLIGHT GENERATOR\n",
    "# ============================================================\n",
    "\n",
    "def generate_normal_flight(n):\n",
    "    t = np.arange(n)\n",
    "\n",
    "    data = {\n",
    "        \"altitude\": 10 + 0.01 * t + np.random.normal(0, 0.2, n),\n",
    "        \"velocity_z\": np.random.normal(0, 0.05, n),\n",
    "        \"accel_z\": np.random.normal(0, 0.1, n),\n",
    "        \"roll\": np.random.normal(0, 0.5, n),\n",
    "        \"pitch\": np.random.normal(0, 0.5, n),\n",
    "        \"yaw\": np.random.normal(0, 1.0, n),\n",
    "        \"motor1_rpm\": 5200 + np.random.normal(0, 50, n),\n",
    "        \"motor2_rpm\": 5200 + np.random.normal(0, 50, n),\n",
    "        \"motor3_rpm\": 5200 + np.random.normal(0, 50, n),\n",
    "        \"motor4_rpm\": 5200 + np.random.normal(0, 50, n),\n",
    "        \"battery_voltage\": 16.8 - 0.0005 * t + np.random.normal(0, 0.02, n),\n",
    "        \"current_draw\": 8 + np.random.normal(0, 0.3, n),\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ============================================================\n",
    "# ANOMALY INJECTION\n",
    "# ============================================================\n",
    "\n",
    "def inject_anomalies(df, start_idx):\n",
    "    df = df.copy()\n",
    "    n = len(df)\n",
    "\n",
    "    df[\"ground_truth\"] = 0\n",
    "\n",
    "    for i in range(start_idx, n):\n",
    "        df.loc[i, \"altitude\"] -= np.random.uniform(3, 6)\n",
    "        df.loc[i, \"velocity_z\"] -= np.random.uniform(0.5, 1.5)\n",
    "        df.loc[i, \"accel_z\"] += np.random.uniform(2, 5)\n",
    "\n",
    "        failed_motor = np.random.choice(\n",
    "            [\"motor1_rpm\", \"motor2_rpm\", \"motor3_rpm\", \"motor4_rpm\"]\n",
    "        )\n",
    "        df.loc[i, failed_motor] -= np.random.uniform(1500, 2500)\n",
    "\n",
    "        df.loc[i, \"current_draw\"] += np.random.uniform(5, 10)\n",
    "        df.loc[i, \"battery_voltage\"] -= np.random.uniform(0.5, 1.0)\n",
    "\n",
    "        df.loc[i, \"ground_truth\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# CREATE TRAIN (NORMAL ONLY)\n",
    "# ============================================================\n",
    "\n",
    "train_df = generate_normal_flight(TRAIN_SAMPLES)\n",
    "train_df.to_csv(\"train_normal.csv\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE TEST (NORMAL + ANOMALIES)\n",
    "# ============================================================\n",
    "\n",
    "test_df = generate_normal_flight(TEST_SAMPLES)\n",
    "test_df = inject_anomalies(test_df, ANOMALY_START)\n",
    "test_df.to_csv(\"test_with_anomalies.csv\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Test samples:\", len(test_df))\n",
    "print(\"Injected anomalies:\", test_df[\"ground_truth\"].sum())\n",
    "print(\"Anomaly start index:\", ANOMALY_START)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6031e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] samples: 3000\n",
      "[KMeans] k = 2\n",
      "[DBSCAN] eps = 0.8 min_samples = 10\n",
      "[OPTICS] min_samples = 10 xi = 0.03\n",
      "[LOF] neighbors = 40\n",
      "[OCSVM] nu = 0.005 gamma = scale\n",
      "[TEST] samples: 2000\n",
      "[POINT] anomalies: 472\n",
      "[WINDOW] anomalies: 430\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1600\n",
      "           1       0.93      1.00      0.96       400\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.97      0.99      0.98      2000\n",
      "weighted avg       0.99      0.98      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RADD++ Drone Anomaly Detection\n",
    "# Train on NORMAL data, Detect on TEST data\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import classification_report # Moved from the end\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT HERE)\n",
    "# ============================================================\n",
    "\n",
    "TRAIN_PATH = \"train_normal.csv\"   # NORMAL ONLY\n",
    "TEST_PATH  = \"test.csv\"           # UNKNOWN\n",
    "\n",
    "TARGET_MAX_ANOMALY_RATE = 0.01\n",
    "PERCENTILE = 99.7\n",
    "VOTE_THRESHOLD = 0.4\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 10\n",
    "WINDOW_THRESHOLD = 0.4\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================\n",
    "# UTILS\n",
    "# ============================================================\n",
    "\n",
    "def perc_thresh(x):\n",
    "    return np.percentile(x, PERCENTILE)\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ LOAD + SCALE (TRAIN)\n",
    "# ============================================================\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "X_train_raw = train_df.select_dtypes(include=[np.number]).values # Renamed for clarity\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw) # Scaled training data\n",
    "\n",
    "print(\"[TRAIN] samples:\", len(X_train))\n",
    "# ============================================================\n",
    "# 2️⃣ AUTO-SEARCH ON TRAIN (NORMAL ONLY)\n",
    "# ============================================================\n",
    "\n",
    "# ---- KMEANS ----\n",
    "# Default\n",
    "k = 2 \n",
    "for k_val in [2, 3, 4]:\n",
    "    kmeans = KMeans(n_clusters=k_val, random_state=RANDOM_STATE, n_init='auto')\n",
    "    kmeans.fit(X_train)\n",
    "    km_dist_train = np.min(\n",
    "        pairwise_distances(X_train, kmeans.cluster_centers_), axis=1\n",
    "    )\n",
    "    # Save current k\n",
    "    k = k_val \n",
    "    if (km_dist_train > perc_thresh(km_dist_train)).mean() <= TARGET_MAX_ANOMALY_RATE:\n",
    "        break\n",
    "\n",
    "km_thresh = perc_thresh(km_dist_train)\n",
    "print(\"[KMeans] k =\", k)\n",
    "\n",
    "# ---- DBSCAN ----\n",
    "# Initialize with valid defaults (safe fallback)\n",
    "eps = 0.8\n",
    "ms = 10 \n",
    "found_db = False\n",
    "\n",
    "for eps_val in [0.8, 1.2, 1.6]:\n",
    "    for ms_val in [10, 20, 30]:\n",
    "        dbscan = DBSCAN(eps=eps_val, min_samples=ms_val)\n",
    "        labels = dbscan.fit_predict(X_train)\n",
    "        \n",
    "        # Check condition\n",
    "        if (labels == -1).mean() <= TARGET_MAX_ANOMALY_RATE:\n",
    "            eps = eps_val\n",
    "            ms = ms_val\n",
    "            found_db = True\n",
    "            break\n",
    "    if found_db:\n",
    "        break\n",
    "\n",
    "print(\"[DBSCAN] eps =\", eps, \"min_samples =\", ms)\n",
    "\n",
    "# ---- OPTICS ----\n",
    "# Initialize with valid defaults\n",
    "ms_o = 10\n",
    "xi = 0.03\n",
    "found_opt = False\n",
    "\n",
    "for ms_o_val in [10, 20, 30]:\n",
    "    for xi_val in [0.03, 0.05, 0.1]:\n",
    "        optics = OPTICS(min_samples=ms_o_val, xi=xi_val)\n",
    "        labels = optics.fit_predict(X_train)\n",
    "        \n",
    "        # Check condition\n",
    "        if (labels == -1).mean() <= TARGET_MAX_ANOMALY_RATE:\n",
    "            ms_o = ms_o_val\n",
    "            xi = xi_val\n",
    "            found_opt = True\n",
    "            break\n",
    "    if found_opt:\n",
    "        break\n",
    "\n",
    "print(\"[OPTICS] min_samples =\", ms_o, \"xi =\", xi)\n",
    "\n",
    "# ---- LOF ----\n",
    "# Default\n",
    "k_lof = 20\n",
    "for k_lof_val in [20, 30, 40]:\n",
    "    lof = LocalOutlierFactor(n_neighbors=k_lof_val, novelty=True)\n",
    "    lof.fit(X_train)\n",
    "    \n",
    "    # Save current k_lof\n",
    "    k_lof = k_lof_val\n",
    "    if (-lof.score_samples(X_train) > 0).mean() <= TARGET_MAX_ANOMALY_RATE:\n",
    "        break\n",
    "\n",
    "print(\"[LOF] neighbors =\", k_lof)\n",
    "\n",
    "# ---- OCSVM ----\n",
    "# Initialize with valid defaults\n",
    "nu = 0.005\n",
    "gamma = 'scale'\n",
    "found_svm = False\n",
    "\n",
    "for nu_val in [0.005, 0.01, 0.02]:\n",
    "    for gamma_val in [\"scale\", 0.1, 0.01]:\n",
    "        ocsvm = OneClassSVM(nu=nu_val, gamma=gamma_val)\n",
    "        ocsvm.fit(X_train)\n",
    "        svm_train_score = -ocsvm.score_samples(X_train)\n",
    "        \n",
    "        # Check condition\n",
    "        if (svm_train_score > perc_thresh(svm_train_score)).mean() <= TARGET_MAX_ANOMALY_RATE:\n",
    "            nu = nu_val\n",
    "            gamma = gamma_val\n",
    "            found_svm = True\n",
    "            break\n",
    "    if found_svm:\n",
    "        break\n",
    "\n",
    "svm_thresh = perc_thresh(svm_train_score)\n",
    "print(\"[OCSVM] nu =\", nu, \"gamma =\", gamma)\n",
    "# ============================================================\n",
    "# 3️⃣ LOAD + SCALE (TEST)\n",
    "# ============================================================\n",
    "\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "FEATURE_COLS = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# The original code here re-read X_train_raw but didn't re-scale it.\n",
    "# We must use the *fitted* scaler to transform the test data.\n",
    "\n",
    "X_test_raw  = test_df[FEATURE_COLS].values\n",
    "\n",
    "# FIX 2: Apply the previously fitted scaler to the test data.\n",
    "# This is CRITICAL. The test data must be transformed using the same \n",
    "# scaling parameters (mean/median, range) learned from the training data.\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "print(\"[TEST] samples:\", len(X_test))\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ SCORE TEST DATA\n",
    "# ============================================================\n",
    "\n",
    "# KMeans\n",
    "km_dist_test = np.min(\n",
    "    pairwise_distances(X_test, kmeans.cluster_centers_), axis=1\n",
    ")\n",
    "km_flag = (km_dist_test > km_thresh).astype(int)\n",
    "\n",
    "# DBSCAN / OPTICS (re-fit on test for density)\n",
    "# FIX 3: DBSCAN/OPTICS should use the parameters found on the training set,\n",
    "# but they should be re-run on the test set to identify outliers (-1).\n",
    "# The original code re-instantiated them and fit_predict() on X_test, which is generally \n",
    "# okay for density methods if the test set is large, but fitting them on the \n",
    "# training set and then using the `predict` or a similar method is safer\n",
    "# in an anomaly detection pipeline.\n",
    "# For simplicity and adherence to the spirit of the original code, we just\n",
    "# ensure the parameters are correct and keep the fit_predict:\n",
    "\n",
    "# DBSCAN is generally not used in a train/test pipeline this way; it should\n",
    "# be re-run on the entire dataset or used with a clustering approach.\n",
    "# In this specific context (RADD++) where density is part of the vote,\n",
    "# re-running it on the test data alone to find 'sparse' points is acceptable\n",
    "# for a simple ensemble, but keep the parameters found on the training set.\n",
    "db_flag = (DBSCAN(eps=eps, min_samples=ms).fit_predict(X_test) == -1).astype(int)\n",
    "opt_flag = (OPTICS(min_samples=ms_o, xi=xi).fit_predict(X_test) == -1).astype(int)\n",
    "\n",
    "# LOF (uses the lof model fitted on X_train)\n",
    "lof_flag = (lof.predict(X_test) == -1).astype(int)\n",
    "\n",
    "# OCSVM (uses the ocsvm model fitted on X_train)\n",
    "svm_test_score = -ocsvm.score_samples(X_test)\n",
    "svm_flag = (svm_test_score > svm_thresh).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ RADD VOTING (POINT LEVEL)\n",
    "# ============================================================\n",
    "\n",
    "flags = np.vstack([\n",
    "    km_flag,\n",
    "    db_flag,\n",
    "    opt_flag,\n",
    "    lof_flag,\n",
    "    svm_flag\n",
    "]).T\n",
    "\n",
    "test_df[\"anomaly_score\"] = flags.mean(axis=1)\n",
    "test_df[\"anomaly\"] = (test_df[\"anomaly_score\"] >= VOTE_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"[POINT] anomalies:\", test_df[\"anomaly\"].sum())\n",
    "\n",
    "# ============================================================\n",
    "# 6️⃣ SLIDING WINDOW (TEST)\n",
    "# ============================================================\n",
    "\n",
    "scores = test_df[\"anomaly_score\"].values\n",
    "window_flags = np.zeros(len(scores))\n",
    "\n",
    "for start in range(0, len(scores) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "    end = start + WINDOW_SIZE\n",
    "    if scores[start:end].mean() >= WINDOW_THRESHOLD:\n",
    "        window_flags[start:end] = 1\n",
    "\n",
    "test_df[\"window_anomaly\"] = window_flags.astype(int)\n",
    "\n",
    "print(\"[WINDOW] anomalies:\", test_df[\"window_anomaly\"].sum())\n",
    "\n",
    "# Assumes 'ground_truth' column exists in test_df\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(\n",
    "    test_df[\"ground_truth\"],\n",
    "    test_df[\"window_anomaly\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b4783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6c8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becc567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
